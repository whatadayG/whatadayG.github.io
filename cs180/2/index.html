<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>CS180 Project 1</title>
    <link rel="StyleSheet" href="./main.css" type="text/css" media="all">
  </head>
  <body>
    <h1>
      CS180 Project 2
    </h1>
    <h1>
      Results
    <!--
    <img src="./output/church.jpg" height="400" width="400">
    <img src="./output/emir.jpg" height="400" width="400">
    <img src="./output/harvesters.jpg" height="400" width="400">
    <img src="./output/icon.jpg" height="400" width="400">
    <img src="./output/lady.jpg" height="400" width="400">
    <img src="./output/melons.jpg" height="400" width="400">
    <img src="./output/monastery.jpg" height="400" width="400">
    <img src="./output/onion_church.jpg" height="400" width="400">
    <img src="./output/sculpture.jpg" height="400" width="400">
    <img src="./output/self_portrait.jpg" height="400" width="400">
    <img src="./output/three_generations.jpg" height="400" width="400">
    <img src="./output/train.jpg" height="400" width="400">
    -->
      <h2>
        <a>1.1: Finite Difference Operator</a>
        <h3>
          <a>Convolution with Finite Difference in the x direction</a>

          <img src="./images/1.1/partial_x.png" height="540" width="542">
        </h3>
        <h3>
          <a>Convolution with Finite Difference in the y direction</a>
          <img src="./images/1.1/partial_y.png" height="540" width="542">
        </h3>
        <h3>
          <a>Gradient Magnitude Image</a>
          <img src="./images/1.1/gradient.png" height="540" width="542">
        </h3>
        <h3>
          <a>Binarized (Edge Image)</a>
          <img src="./images/1.1/edge.png" height="540" width="542">
        </h3>
      </h2>

      <h2>
        <a>1.2: Derivative of Gaussian Filter</a>
        <h3>
          <a>Convolution with Gaussian Kernel (Blurred Image)</a>
          <img src="./images/1.2/cameraman.png" height="540" width="542">
        </h3>
        <h3>
          <a>Convolution of Blurred Image with D_x</a>
        </h3>
        <h3>
          <a>Convolution of Blurred Image with D_y</a>
        </h3>
        <h3>
          <a>Blurred Gradient Magnitude</a>
        </h3>
        <h3>
          <a>Edge Image </a>
        </h3>
        Note above that the edge image on the blurred image is much less noisy than before.
        Additionally, the edges are typically thicker and have fewer discontinuities than when using the difference filter on the original iamge.

        <h3>
          <a>Gaussian Convolved with D_x</a>
          <img src="./images/1.2/DoG_x.png" height="480" width="640">
        </h3>
        <h3>
          <a>Gaussian Convolved with D_y</a>
          <img src="./images/1.2/DoG_y.png" height="480" width="640">
        </h3>
          <a>Original Gaussian Filter</a>
          <img src="./images/1.2/2D Gaussian Filter.png" height="480" width="640">
        </h3>
        <!--
        <h3>
          <a>Convolution with Horizontal DoG Filter</a>
        </h3>
        <h3>
          <a>Convolution with Vertical DoG Filter</a>
        </h3>
        <h3>
          <a>Blurred Gradient Magnitude with DoG</a>
        </h3>
        <h3>
          <a>Binarized (Threshold=)</a>
        </h3>
        -->
        As expected, the results of convolving twice (first with the Gaussian and then the D_x/D_y) filters are the same as the results of convolving with the combined filter.
      </h2>
      <h2>
        <a>2.1: Image "Sharpening"</a>
      </h2>
        <h3>
          <a>Taj Mahal</a>
          <img src="./images/2.1/blurred_castle.png" height="853" width="853">
          <img src="./images/2.1/sharpened_castle.png" height="853" width="853">
        </h3>
        <h3>
          <a>This bird</a>
          <img src="./images/2.1/blurred_bird.png" height="850" width="1280">
          <img src="./images/2.1/sharpened_bird.png" height="850" width="1280">
        </h3>
        <h3>
          <a>These boats</a>
          <img src="./images/2.1/blurred_folkstone.png" height="850" width="1280">
          <img src="./images/2.1/sharpened_folkstone.png" height="850" width="1280">
        </h3>
        <h3>
          <a>Blurring and unblurring a sharp image</a>
        </h3>
        <h3>
          <a>Discussion</a>
        </h3>
      <h2>
        <a>2.2: Hybrid Images</a>
        <h3>
          <a>Log magnitudes of FFTs of an image of a boy</a>
          <img src="./images/2.2/FFT of Image boy.png" height="480" width="640">
        </h3>
        <h3>
          <a>Low pass filtered version</a>
          <img src="./images/2.2/FFT of Image boy lowpass.png" height="480" width="640">
        </h3>
        <h3>
          <a>Log magnitudes of FFTs of an image of an owl</a>
          <img src="./images/2.2/FFT of Image owl.png" height="480" width="640">
        </h3>
        <h3>
          <a>High pass filtered version</a>
          <img src="./images/2.2/FFT of Image owl highpass.png" height="480" width="640">
        </h3>
        <h3>
          <a>Sum of the above low pass filtered and high pass filtered version</a>
          <img src="./images/2.2/FFT of Hybrid Image.png" height="480" width="640">
        </h3>
        <h3>
          <a>Hybrid Image</a>
          <img src="./images/2.2/Owl_Boy_bw.png" height="480" width="640">
        </h3>
        <h3>
          <a>Tiger and a dog</a>
          <img src="./images/2.2/Tiger_Dog_bw.png" height="480" width="640">
        </h3>
        <h3>
          <a>Derek and Nutmeg</a>
          <img src="./images/2.2/hybrid_bw.png" height="480" width="640">
        </h3>
        <h3>
          <a>Other examples</a>
        </h3>
      </h2>
      <h2>
        <a>2.3: Gaussian and Laplacian Stacks</a>
        <h3>
          <a>Recreation of the Szelski paper</a>
        </h3>
      </h2>
      <h2>
        <a>2.4: Multiresolution Blending</a>
      </h2>
        <h3>
          <a>Vertical seam</a>
        </h3>
        <h3>
          <a>Irregular mask</a>
        </h3>
        <h3>
          <a>This thing</a>
        </h3>
    </h1>

    <h1>
      Reflections
    </h1>
    <p>
    Discussion here.
    </p>
    <p>
      In order to achieve this, I mostly followed the methods suggested in the project page, which worked mostly without complication.
      Specifically, I implemented a search over all offsets to align the green and red images to the blue image.
      8 percent of the borders of the images are removed to avoid the dark borders causing trouble with the alignment metric.
      The quality of the offset is judged by the sum of squared differences in pixels, also as suggested.
    </p>
    <p>
      Some problems I ran into in the course of developing these results include:
      <ul>
      <li>For a long time I thought I had a bug where the wrong image was being moved when just trying to align the green image to the blue, causing me to question my sanity.
      This was actually because I was misinterpreting the image I was using for debugging: areas which are black on the green image would look blue, because of course the green-value was zero and the blue-value was nonzero.</li>
      <li>The alignment, even for low-res images, will not work if the borders are not cropped.</li>
      <li>For the emir picture in particular, the fact that his jacket is blue means that the l2 metric actually
        isn't very good for aligning the red image to the blue image, since the blue values are high and the red values are low.
        Originally, this resulted in complete failure when the image was cropped to the middle third in both dimensions.
        This was resolved by reducing the extent of the cropping to just 8% of the image dimension per side, so that the L2 norm could be applied to other areas of the image.
        Of course, this is not a fully general solution, and the emir photo still has clearly the worst alignment of the ones in this set.
        In general it would probably be better to use an edge detector or a combination of metrics.
      </li>
      <li>Naturally, the initial for-loop implementation of the L2 difference is way too slow to be useful. Replacing it with a numpy call made things something like 50-200 times faster, which was good enough.</li>
      <li>It's technically only necessary to search a 3 pixel range in the intermediate calls of the pyramid search to find the best alignment, assuming the "greedy" solution is best.
        However, there was some concern that the greediness assumption wouldn't hold, so I increased the range to 7 pixels just in case.
      </li>
      </ul>
    </p>
    <p>As a subjective matter, I think (aside from removing the colored borders), the biggest problem with the colorized images is the color balance. Some of the images, particularly the ones with sky or water, look either kind of washed out or gray. As mentioned on the website, it might be good to try and decompose the current (R, G, B) as a combination of some other (R', G', and B'). But it's not clear how to do this without obsessively trying a bunch of combinations manually.</p>

</body></html>
