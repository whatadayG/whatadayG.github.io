<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>CS180 Project 4</title>
    <link rel="StyleSheet" href="./main.css" type="text/css" media="all">
  </head>
  <body>
    <h1>
      CS180 Project 5
    </h1>
    <h1>
      <h2>
        <a>Using a Diffusion Model to Generate Images</a>
      </h2>
        
      
        <h2>0: Sampling</h2>
        Here are some samples generated by the DeepFloyd IF model, along with the captions used to generate them.
        NOTE: In this project, all images were generated with random seed 1725. (These three images may have been generated with seed 181. Sorry!)
        Caption: an oil painting of a snowy mountain village
        <img src="./images/sample_1.png" height="400" width="400">
        Caption: a man wearing a hat
        <img src="./images/sample_2.png" height="400" width="400">
        Caption: a rocket ship
        <img src="./images/sample_3.png" height="400" width="400">

      <h2>1: Adding Noise</h2>

        In this step we add increasing levels of noise to an image of the Berkeley Campanile, and parameterize the noise level with an integer ranging from 0 to 1000, following the original paper. We'll see later what the model is able to do about this.
        
        Noise level 0:
        <img src="./images/campanile.png" height="400" width="400">
        
        Noise level 250:
        <img src="./images/campanile_250.png" height="400" width="400">
        
        Noise level 500:
        <img src="./images/campanile_500.png" height="400" width="400">
        
        Noise level 750:
        <img src="./images/campanile_750.png" height="400" width="400">
        
        A noise level of 1000 corresponds to a completely random image, where the pixel values are sampled from a Gaussian.



      
        <h2>2. Classical Denoising</h2>
        Here's an attempt at denoising using a Gaussian filter, the way we learned to for project 2:
        Noise level 250:
        <div class="grid-container">
          <img src="./images/campanile_250.png" height="400" width="400">
          <img src="./images/gaussian_250.png" height="400" width="400">
        </div>
        
        Noise level 500:
        <div class="grid-container">
          <img src="./images/campanile_500.png" height="400" width="400">
          <img src="./images/gaussian_500.png" height="400" width="400">
        </div>
        
        Noise level 750:
        <div class="grid-container">
          <img src="./images/campanile_750.png" height="400" width="400">
          <img src="./images/gaussian_750.png" height="400" width="400">
        </div>

        As you can see, the results aren't very good, and the blurring doesn't in general remove the staticy look of the noise.

        <h2>3. Denoising with a Diffusion Model (in one step)</h2>
        Here we use a diffusion model to predict the noise, which effectively allows us to predict the original image.
        The original image is found on the right.

        Noise level 250:
        <div class="grid-container">
          <img src="./images/campanile_250.png" height="400" width="400">
          <img src="./images/denoised_250.png" height="400" width="400">
          <img src="./images/campanile.png" height="400" width="400">
        </div>
        
        Noise level 500:
        <div class="grid-container">
          <img src="./images/campanile_500.png" height="400" width="400">
          <img src="./images/denoised_500.png" height="400" width="400">
          <img src="./images/campanile.png" height="400" width="400">
        </div>
        
        Noise level 750:
        <div class="grid-container">
          <img src="./images/campanile_750.png" height="400" width="400">
          <img src="./images/denoised_750.png" height="400" width="400">
          <img src="./images/campanile.png" height="400" width="400">
        </div>

        As you can see, the reconstruction is a lot more natural and image-like.
        (Of course the reconstruction can't be perfect because information is lost to the noising,
        so to make things image-like the model necessarily has to hallucinate some details.)

        <h2>4. Denoising with a Diffusion Model (in multiple steps)</h2>
        Here we instead take advantage of the time-conditioning input to diffusion models, which allows us to iteratively denoise the image.
        This is a lot more computationally expensive, but the result looks nicer because there is more compute around to recreate nice features.

        Here are some of the intermediate results, followed by the final reconstruction:
        <div class="grid-container">
          <img src="./images/seq1.png" height="400" width="400">
          <img src="./images/seq2.png" height="400" width="400">
          <img src="./images/seq3.png" height="400" width="400">
        </div>
        <div class="grid-container">
          <img src="./images/seq4.png" height="400" width="400">
          <img src="./images/seq5.png" height="400" width="400">
          <img src="./images/seq6.png" height="400" width="400">
        </div>

        And on the same noise level, here is the one-shot model attempt, the classical Gaussian filter attempt, and the original image:
        <div class="grid-container">
          <img src="./images/seq_oneshot.png" height="400" width="400">
          <img src="./images/seq_gaussian.png" height="400" width="400">
          <img src="./images/campanile.png" height="400" width="400">
        </div>

        As described, the multiple-step model creates a much better looking image, though with this much noise it had to hallucinate a lot and
        so we actually have a picture of a substantially different tower now.

        <h2>5. Sampling With a Diffusion Model</h2>
        Given that the model removes noise from an image, what happens if we feed in an
        image of pure noise? (Well, we can generate a new image from scratch.)

        This particular model actually also has a text-conditioning feature, which allows us to input a prompt to condition the generation on.
        For these generations, we gave the generic prompt "a high quality image".
        <div class="grid-container">
          <img src="./images/5_normal_samples.png" height="400" width="400">
        </div>
        These things look vaguely like real images, but they are very blurry and have weird artifacts.

        <h2>6. Classifier-Free Guidance</h2>
        The text conditioning affects the noise estimate output by the model.
        If we compare this estimate to the estimate from a baseline text prompt (literally "", the empty string),
        we get a measure of what the text conditioning does. If we simply increase the weight of the text conditioning,
        we can increase its effect, and improve the quality of the image.
        Here is the same process as the above, but with the text conditioning weight increased by a factor of 5:
        <div class="grid-container">
          <img src="./images/5_better_samples.png" height="400" width="400">
        </div>
        These images are clearly way better, and look like actual images.

        <h2>7.Image-to-Image Translation</h2>
        In section 4 above, we saw that when we added noise to an image, when the model denoised it,
        we got a slightly different image than the original.
        We can do this intentionally to create images of varying degrees of similarity to a given image, for fun.

        Here we do this with the picture of the Campanile again. On a sort of scale from 1-33 in degree of similarity to the original,
        here are images of similarity 1, 3, 5, 7, 10, and 20:
        <div class="grid-container">
          <img src="./images/similar_campanile_1.png" height="400" width="400">
        </div>
        <div class="grid-container">
          <img src="./images/similar_campanile_2.png" height="400" width="400">
        </div>

        Now we repeat this with this cool penguin, as well as some custom hand-drawn images:
        <img src="./images/based_penguin.png" height="400" width="400">
        <div class="grid-container">
          <img src="./images/similar_penguin_1.png" height="400" width="400">
        </div>
        <div class="grid-container">
          <img src="./images/similar_penguin_2.png" height="400" width="400">
        </div>

        <img src="./images/cat_drawing.jpg" height="400" width="400">
        <div class="grid-container">
          <img src="./images/similar_cat_1.png" height="400" width="400">
        </div>
        <div class="grid-container">
          <img src="./images/similar_cat_2.png" height="400" width="400">
        </div>

        <img src="./images/tree_drawing.jpg" height="400" width="400">
        <div class="grid-container">
          <img src="./images/similar_tree_1.png" height="400" width="400">
        </div>
        <div class="grid-container">
          <img src="./images/similar_tree_2.png" height="400" width="400">
        </div>

        We can also inpaint a specific region of an image, by just noising the image and then
        constraining the model's reconstruction to only change the region.
        For example, here we decide to inpaint just the top part of the Campanile:
        <div class="grid-container">
          <img src="./images/inpaint_campanile.png" height="400" width="400">
          <img src="./images/campanile.png" height="400" width="400">
        </div>

        Here, we overwrite the right side of the image to reinterpret this person's stance:
        <div class="grid-container">
          <img src="./images/inpaint_yoga.png" height="400" width="400">
          <img src="./images/yoga.jpg" height="400" width="400">
        </div>

        Here's a slightly ridiculous output of the model which puts the wrong kind of face on this person:
        <div class="grid-container">
          <img src="./images/inpaint_holdinghands.png" height="400" width="400">
          <img src="./images/holding_hands.jpg" height="400" width="400">
        </div>

        Finally, we can also prompt the inpaint to steer the changes to the image in a specific way.
        Here's the photo of the Campanile again, but with the inpaint steered to replace the tower with a rocket.
        As before, we can change the degree of noising to affect the amount of required similarity to the original image.
        
        <img src="./images/campanile.png" height="400" width="400">
        <div class="grid-container">
          <img src="./images/inpaint_rocket_1.png" height="400" width="400">
        </div>
        <div class="grid-container">
          <img src="./images/inpaint_rocket_2.png" height="400" width="400">
        </div>

        Here's a cat I edited to look like a dog.
        <img src="./images/cat.jpg  " height="400" width="400">
        <div class="grid-container">
          <img src="./images/inpaint_dog_1.png" height="400" width="400">
        </div>
        <div class="grid-container">
          <img src="./images/inpaint_dog_2.png" height="400" width="400">
        </div>

        Here's a guy I edited to have a hat on.
        <img src="./images/face.jpg" height="400" width="400">
        <div class="grid-container">
          <img src="./images/inpaint_hat_1.png" height="400" width="400">
        </div>
        <div class="grid-container">
          <img src="./images/inpaint_hat_2.png" height="400" width="400">
        </div>

        <h2>8. Visual Anagrams</h2>

        We can pull all sorts of tricks with the steering of the generation.
        In this section, in each iteration of the denoising, we reconstruct the image according to one prompt, then we
        denoise the image upside down according to another prompt.
        When this works, the result is one of those fun illusions where the image looks like something right side up,
        but something else upside down.

        This is an old man right side up, but people around a campfire upside down:
        <div class="grid-container"></div>
          <img src="./images/illusion_11.png" height="400" width="400">
          <img src="./images/illusion_12.png" height="400" width="400">
        </div>
        This is a skull right side up, but a hipster barista upside down:
        <div class="grid-container"></div>
          <img src="./images/illusion_21.png" height="400" width="400">
          <img src="./images/illusion_22.png" height="400" width="400">
        </div>

        This is a pair of cabins in a snowy village right side up, but a pair of dogs upside down:
        <div class="grid-container"></div>
          <img src="./images/illusion_31.png" height="400" width="400">
          <img src="./images/illusion_32.png" height="400" width="400">
        </div>

        <h2>9. Hybrid Images</h2>
        Likewise since we generate the images by having the model predict the noise,
        we can manipulate the noise with high/low pass filters to create images
        which have (sort of) been steered according to different prompts for different frequencies.

        Here is something that is a waterfall close up, but a skull from a distance:
        <img src="./images/hybrid1.png" height="400" width="400">
        Here is something which is a bunch of people surrounding a fire close up, but a dog from a distance:
        <img src="./images/hybrid2.png" height="400" width="400">

        Here is something which is a rocket ship close up, but a pencil from a distance:
        (I really like this one, because the way of fitting the constraints of both prompts is a little clever:
        the body of the pencil is the exhaust of the ship, and the ship is just the graphite.)
        <img src="./images/hybrid3.png" height="400" width="400">

        This effect isn't always as simple as our interpretation. For example, whether at low or high pass,
        it's not very easy to make a dog look like a man with this number of pixels, and the opposing prompts
        don't do any "planning" to jointly fit the constraints.
        So any attempt to get "a dog close up, but a man from a distance" creates these cursed dog-men.
        <img src="./images/hybrid4.png" height="400" width="400">


        This concludes the messing around with the model.


    </h1>

    <h1>
      Part B: Training a Diffusion Model
    </h1>

      In this section of the project, we train a diffusion model to generate images of handwritten digits,
      based off the classic MNIST dataset.
      The training procedure is pretty straightforward. From the description of the task above, note that all it really takes to create
      training data of a generic type is to apply Gaussian noise to existing data.
      (Text constraints and such are a little more complicated.)

      To start with, here is a sample of the training data, as well as the way the datapoints look at varying levels of noise:
      <img src="./images/partb/sample_data.png" height="600" width="800">

      First, we train a U-net to predict the noise in the images in one shot. (Since this task was easy, the batch size was really large and the number of batches it took to converge was small.)
      Here is the training loss:
      <img src="./images/partb/loss1.png" height="600" width="1100">

      It should be noted that the training data for this particular model consisted only of data that was noised at sigma=0.5.
      Here are some sample denoises from the model after training for 1 epoch:
      <img src="./images/partb/denoise1.png" height="600" width="600">
      and for 5 epochs:
      <img src="./images/partb/denoise5.png" height="600" width="600">

      We also try to use this model to denoise images that were noised at a different sigma.
      Surprisingly, it works really well:
      <img src="./images/partb/denoise_many.png" height="600" width="1200">

      <h2>Time Conditioning</h2>
      But this model, which is intrinsically one-shot, can't be used to perform the iterative denoising process that we used in part A.
      Instead, we need to bake some context into the model that tells it approximately how much noise was added to the image.
      This is called time-conditioning, and we basically achieve it by injecting an embedding of the sigma value into the model.

      As instructed, since this was a much harder task, we trained the model for more epochs and with a different learning rate schedule.
      Here is the training loss for a model that was time-conditioned in this way:
      <img src="./images/partb/loss2.png" height="600" width="1100">

      And here are some sample (single-iteration)attempts from the model to denoise:
      <img src="./images/partb/tc_denoise.png" height="600" width="600">

      Finally, here are some denoised samples from the model after training for 5 epochs:
      <img src="./images/partb/tc5.png" height="600" width="600">

      and for 20 epochs:
      <img src="./images/partb/tc20.png" height="600" width="600">

      <h2>Class Conditioning</h2>
      We can condition the model on a class label as well, by just injecting an embedding of the one-hot label into the model.
      In addition we sometimes inject an embedding of the zero vector, so that the model can still generate the images without being conditioned.
      The fact that the model has a "null" embedding also allows us to do the classifier-free guidance trick, the way we did in part A.

      Here is the loss schedule for this model:
      <img src="./images/partb/loss3.png" height="600" width="1100">

      Here are some sample denoises from the model:
      <img src="./images/partb/cc_denoise.png" height="600" width="600">

      And here are some denoised samples from the model after training for 5 epochs:
      <img src="./images/partb/cc5.png" height="600" width="600">

      and for 20 epochs:
      <img src="./images/partb/cc20.png" height="600" width="600">

</body></html>